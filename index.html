<!doctype html> <html lang=en > <meta charset=UTF-8 > <meta name=viewport  content="width=device-width, initial-scale=1"> <link rel=stylesheet  href="/libs/katex/katex.min.css"> <link rel=stylesheet  href="/css/franklin.css"> <link rel=stylesheet  href="/css/basic.css"> <link rel=icon  href="/assets/favicon.png"> <title>Home</title> <header> <div class=blog-name ><a href="/">Nikola Janjušević</a></div> <nav> <ul> <li><a href="/teaching">Teaching</a> <li><a href="/publications">Publications</a> <li><a href="/notes">Notes</a> <li><a href="/assets/cv.pdf">CV</a> </ul> <img src="/assets/hamburger.svg" id=menu-icon > </nav> </header> <div class=franklin-content ><h2 id=about_me ><a href="#about_me" class=header-anchor >About me</a></h2> <div class=row ><div class=container ><img class=left  id=profpic src="assets/profilepic3.jpg" onmouseover="this.src='assets/profilepicQ.jpg';" onmouseout="this.src='assets/profilepic3.jpg';" > <p>Hello. My name is Nikola Janjušević. I am a fourth year <strong>PhD candidate in Electrical Engineering</strong> at <em>New York University</em> under the advisory of Professor Yao Wang, <a href="https://wp.nyu.edu/videolab/">NYU VideoLab</a>. I received my Bachelors in Electrical Engineering from <em>The Cooper Union for Advancement of Science and Art</em> in 2019.</p> <p>My current interests are in <em>interpretable deep-learning</em> models for solving <em>inverse-problems</em> and low-level <em>computer-vision</em>/<em>image-processing</em> tasks. My background is in <em>signal-processing</em> and <em>non-smooth, convex optimization</em>. Outside of academia, I go biking and skateboarding with my friends.</p></div></div> <h2 id=updates ><a href="#updates" class=header-anchor >Updates</a></h2> <ul> <li><p>October 2023: Presented poster at NYU CAI<span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">^2</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:0.8141079999999999em;vertical-align:0em;"></span><span class=mord ><span></span><span class=msupsub ><span class=vlist-t ><span class=vlist-r ><span class=vlist  style="height:0.8141079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span>R <a href="https://cai2r.net/training/i2i-workshop/">From Innovation to Implementation in Imaging &#40;i2i&#41; Workshop</a>: <a href="/assets/research/i2i_poster_snacdl.pdf">&quot;SNAC-DL: Self-Supervised Network for Adaptive Convolutional Dictionary Learning of MRI Denoising&quot;</a></p> <li><p>October 2023: <a href="https://opg.optica.org/boe/fulltext.cfm?uri&#61;boe-14-11-5539&amp;id&#61;540503"><em>&quot;Compressed sensing on displacement signals measured with optical coherence tomography&quot;</em></a> published in Biomedical Optics Express&#33;</p> <li><p>August 2023: <a href="assets/research/i2iabs_2023.pdf"><em>&quot;Self-Supervised Low-Field MRI Denoising via Spatial Noise Adaptive CDLNet&quot;</em></a> abstract accepted to NYU CAI2R <a href="https://cai2r.net/training/i2i-workshop/">i2i Workshop</a>.</p> <li><p>August 2023: <a href="https://github.com/nikopj/SSIMLoss.jl">SSIMLoss.jl</a>, differentiable SSIM loss functions for neural network training, package published.</p> <li><p>June 2023: Joined NYU Langone Department of Radiology as a Non-Traditional Volunteer Intern, working on unsupervised learning for MRI denoising and CS-MRI reconstruction.</p> <li><p>June 2023: <a href="https://arxiv.org/abs/2306.01950"><em>&quot;Fast and Interpretable Nonlocal Neural Networks for Image Denoising via Group-Sparse Convolutional Dictionary Learning&quot;</em></a> submitted to IEEE Transactions on Image Processing. Preprint available on arxiv.</p> </ul> <h2 id=recent_blog_posts ><a href="#recent_blog_posts" class=header-anchor >Recent blog posts</a></h2> <ul> <li><p><strong><a href="/blog/chain_rule/">&#40;Apr 12 2023&#41; On using the multi-dimensional chain-rule correctly.</a></strong></p> </ul> <p>TLDR: employing the multi-dimensional chain-rule means writing matrix-multiplication. ...</p> <ul> <li><p><strong><a href="/blog/conv/">&#40;Dec 11 2022&#41; How to think of Conv-Layers in Neural Networks</a></strong></p> </ul> <p>Convolutional Neural Networks&#39; building blocks aren&#39;t just performing the convolution you learned in DSP. In my opinion, the best way to think of these layers is as a channel-wise matrix-vector multiplication of convolutions. <img alt="convolution blocks" src="/assets/conv/conv_blocks.svg" style="width:50%" class=center  /> ...</p> <ul> <li><p><strong><a href="/blog/small_nets/">&#40;Dec 11 2022&#41; How do black-box denoisers perform at the lower parameter-count regime, anyway?</a></strong></p> </ul> <p>So-called interpretably constructed deep neural networks often sell their methods by showing near state-of-the-art performance for only a fraction of the parameter count of black-box networks. However, can we consider these fair comparisons when the number of learned parameter counts are not matched? ...</p> <ul> <li><p><strong><a href="/blog/julia_tvd/">&#40;Jun 5 2021&#41; Introduction to Julia by TV denoising</a></strong></p> </ul> <p>A walkthrough of implementing Total Variation color image denoising in the Julia programming language, starring Fabio and Masa. <img src="/assets/tvd/exfabio.png" alt="" /> ...</p> <ul> <li><p><strong><a href="/blog/arxivmv/">&#40;Oct 25 2020&#41; Staying organized while doing research &#40;arxivmv&#41;.</a></strong></p> </ul> <p>I often find my downloads folder filling up with tons of research papers with nondescript &#40;ID&#41; names, such as &quot;1909.05742.pdf&quot;. Keeping these PDFs open allows me to keep track of them, but once I close those windows they seem as good as lost. To remedy this, I&#39;ve written a short Python script employing a <a href="https://github.com/lukasschwab/arxiv.py">wrapper for the arXiv.org API</a>. ...</p> <ul> <li><p><strong><a href="/blog/understanding-ista/">&#40;Feb 29 2020&#41; Understanding ISTA as a Fixed-Point Iteration</a></strong></p> </ul> <p>The iterative soft thresholding algorithm is one of the simplest algorithms for sparse coding &#40;in this case, solving the basis-pursuit denoising functional&#41;. Understanding its derivation as a special case of the Proximal Gradient Method is a great introduction into the world of <strong>proximal methods</strong>. ...</p> <ul> <li><p><strong><a href="/blog/auto-reload-latex/">&#40;Jan 3 2020&#41; My auto-reload setup for writing LATEX documents in VIM</a></strong></p> </ul> <p>Zathura &#43; latexmk -&gt; :&#41;. Latest update: 17th January 2021. ...</p> <div class=page-foot > <div class=copyright > <!--<a href="https://github.com/nikopj">GitHub</a> <br> --> &copy; <a href="https://github.com/nikopj">Nikola Janjušević</a>. Last modified: 0001-01-01. Built with <a href="https://github.com/tlienart/Franklin.jl">Franklin.jl</a> + <a href="https://julialang.org">Julia</a>. </div> </div> </div>