<!doctype html> <html lang=en > <meta charset=UTF-8 > <meta name=viewport  content="width=device-width, initial-scale=1"> <link rel=stylesheet  href="/libs/katex/katex.min.css"> <link rel=stylesheet  href="/css/franklin.css"> <link rel=stylesheet  href="/css/basic.css"> <link rel=icon  href="/assets/favicon.png"> <title>Deep Convolutional Dictionary Learning</title> <header> <div class=blog-name ><a href="">Nikola Janjušević</a></div> <nav> <ul> <li><a href="/">Home</a> <li><a href="/projects">Projects</a> <li><a href="/assets/resume.pdf">Résumé</a> </ul> <img src="/assets/hamburger.svg" id=menu-icon > </nav> </header> <div class=franklin-content > <h1 id=title ><a href="#title" class=header-anchor >Deep Convolutional Dictionary Learning</a></h1> <figure style="text-align:center;"> <img src="/assets/dcdl/cdlnet_blockdiagram2.png" style="padding:0; width:100%" alt=" CDLNet Block Diagram."/> <figcaption> CDLNet Block Diagram.</figcaption> </figure> <h3 id=latest_update ><a href="#latest_update" class=header-anchor >Latest Update:</a></h3> <p>December 2021: Journal article preprint, <em>&quot;CDLNet: Noise-Adaptive Convolutional Dictionary Learning Network for Blind Denoising and Demosaicing&quot;</em>, avilable: <a href="https://arxiv.org/abs/2112.00913">https://arxiv.org/abs/2112.00913</a>.</p> <div class=franklin-toc ><ol><li><a href="#project_overview">Project Overview</a><li><a href="#highlight_generalization_in_denoising">Highlight: Generalization in Denoising</a><li><a href="#progress">Progress</a></ol></div> <h2 id=project_overview ><a href="#project_overview" class=header-anchor >Project Overview</a></h2> <p>Sparse representation is a proven and powerful prior for natural images and restoration tasks &#40;such as denoising, deblurring, in-painting, etc.&#41; involving them. More than simply finding these representations, learning an over-complete dictionary for sparse signal representation from degraded signals have been shown to be effective models. Furthermore, the convolutional dictionary learning &#40;CDL&#41; model seeks to represent the global signal via a translated local dictionary. This offers a more holistic approach for natural image representation compared to inherently suboptimal patch-processing methods. The dictionary learning problem is traditionally solved by iteratively compute spare-codes &#40;representations&#41; for a fixed dictionary, and subsequently updating the dictionary accordingly. </p> <p>In this project, <strong>we explore an interpretable Deep Learning architecture for image restoration based on an unrolled CDL model</strong>. More specifically, we leverage the LISTA framework to obtain approximate convolutional sparse codes, followed by a synthesis from a convolutional dictionary. We call this architecture <strong>CDLNet</strong>. The network is trained in a task-driven fashion, amenable to any linear inverse-problem. Interpretability of such networks allow us to further extend the framework for the specific signal class and generalization requirements at hand.</p> <h3 id=participants ><a href="#participants" class=header-anchor >Participants</a></h3> <ul> <li><p>Yao Wang, Advising Professor, <a href="https://wp.nyu.edu/videolab/">Lab Page</a></p> <li><p><a href="https://nikopj.github.io">Nikola Janjušević</a>, Ph.D. student</p> <li><p><a href="https://amirhkhalilian.github.io/">Amir Khalilian</a>, Ph.D. student</p> </ul> <h2 id=highlight_generalization_in_denoising ><a href="#highlight_generalization_in_denoising" class=header-anchor >Highlight: Generalization in Denoising</a></h2> <p>The derivation of the CDLNet architecture allows us to understand the subband thresholds, <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>τ</mi><mrow><mo stretchy=false >(</mo><mi>k</mi><mo stretchy=false >)</mo></mrow></msup><mo>∈</mo><msubsup><mi mathvariant=double-struck >R</mi><mo>+</mo><mi>M</mi></msubsup></mrow><annotation encoding="application/x-tex">\tau^{(k)} \in \mathbb R_+^M</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:0.9270999999999999em;vertical-align:-0.0391em;"></span><span class=mord ><span class="mord mathnormal" style="margin-right:0.1132em;">τ</span><span class=msupsub ><span class=vlist-t ><span class=vlist-r ><span class=vlist  style="height:0.8879999999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span><span class="mclose mtight">)</span></span></span></span></span></span></span></span></span><span class=mspace  style="margin-right:0.2777777777777778em;"></span><span class=mrel >∈</span><span class=mspace  style="margin-right:0.2777777777777778em;"></span></span><span class=base ><span class=strut  style="height:1.146662em;vertical-align:-0.305331em;"></span><span class=mord ><span class="mord mathbb">R</span><span class=msupsub ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.8413309999999999em;"><span style="top:-2.4530000000000003em;margin-left:0em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mbin mtight">+</span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.10903em;">M</span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.305331em;"><span></span></span></span></span></span></span></span></span></span>, of the soft-thresholding operator as implicitly being a function of the input noise-level <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>σ</mi></mrow><annotation encoding="application/x-tex">\sigma</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">σ</span></span></span></span>. We thus propose an affine parameterization,</p> <span class=katex-display ><span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML" display=block ><semantics><mrow><msup><mi>τ</mi><mrow><mo stretchy=false >(</mo><mi>k</mi><mo stretchy=false >)</mo></mrow></msup><mo>=</mo><msubsup><mi>τ</mi><mn>0</mn><mrow><mo stretchy=false >(</mo><mi>k</mi><mo stretchy=false >)</mo></mrow></msubsup><mo>+</mo><msubsup><mi>τ</mi><mn>1</mn><mrow><mo stretchy=false >(</mo><mi>k</mi><mo stretchy=false >)</mo></mrow></msubsup><mi>σ</mi></mrow><annotation encoding="application/x-tex"> \tau^{(k)} = \tau_0^{(k)} + \tau_1^{(k)}\sigma </annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:0.938em;vertical-align:0em;"></span><span class=mord ><span class="mord mathnormal" style="margin-right:0.1132em;">τ</span><span class=msupsub ><span class=vlist-t ><span class=vlist-r ><span class=vlist  style="height:0.938em;"><span style="top:-3.113em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span><span class="mclose mtight">)</span></span></span></span></span></span></span></span></span><span class=mspace  style="margin-right:0.2777777777777778em;"></span><span class=mrel >=</span><span class=mspace  style="margin-right:0.2777777777777778em;"></span></span><span class=base ><span class=strut  style="height:1.311108em;vertical-align:-0.266308em;"></span><span class=mord ><span class="mord mathnormal" style="margin-right:0.1132em;">τ</span><span class=msupsub ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:1.0448em;"><span style="top:-2.433692em;margin-left:-0.1132em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span><span style="top:-3.2198em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span><span class="mclose mtight">)</span></span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.266308em;"><span></span></span></span></span></span></span><span class=mspace  style="margin-right:0.2222222222222222em;"></span><span class=mbin >+</span><span class=mspace  style="margin-right:0.2222222222222222em;"></span></span><span class=base ><span class=strut  style="height:1.311108em;vertical-align:-0.266308em;"></span><span class=mord ><span class="mord mathnormal" style="margin-right:0.1132em;">τ</span><span class=msupsub ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:1.0448em;"><span style="top:-2.433692em;margin-left:-0.1132em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span><span style="top:-3.2198em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span><span class="mclose mtight">)</span></span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.266308em;"><span></span></span></span></span></span></span><span class="mord mathnormal" style="margin-right:0.03588em;">σ</span></span></span></span></span> <p>to <em>explicitly model noise-level adaptivity within each layer of the network</em>. This is in stark contrast to the <em>implicitly defined noise-level adaptivity of common black-box neural networks</em>, which either account for noise only via training on a noise range &#40;ex. DnCNN&#41;, or additionally presented the estimated input noise-level as an input to the network &#40;ex. FFDNet&#41;. As shown in the figures below, CDLNet&#39;s explicitly defined noise-level adaptivity allows for near-perfect generalization outside its training range, whereas the black box models either fail or introduce articfacts.</p> <p> <figure style="text-align:center;"> <img src="/assets/dcdl/gray_generalize_plot.png" style="padding:0; width:65%" alt=" CDLNet is able to generalize outside its training noise-level range, whereas black-box neural networks fail."/> <figcaption> CDLNet is able to generalize outside its training noise-level range, whereas black-box neural networks fail.</figcaption> </figure> <figure style="text-align:center;"> <img src="/assets/dcdl/gray_visual.png" style="padding:0; width:100%" alt=" "/> <figcaption> </figcaption> </figure> </p> <p>This generalization characteristic is further demonstrated for the CDLNet architecture extended to color image denoising, joint-denoising-and-demosiacing, and unsupervised learning of denoising.</p> <h3 id=joint_denoising_and_demosaicing ><a href="#joint_denoising_and_demosaicing" class=header-anchor >Joint Denoising and Demosaicing</a></h3> <p>CDLNet extended to the JDD task is able to achieve state-of-the-art results with a single model, out-performing black box neural networks. <figure style="text-align:center;"> <img src="/assets/dcdl/JDD_table.png" style="padding:0; width:90%" alt=" PSNR Comparison against state-of-the-art JDD models on the Urban100/MIT moire datasets"/> <figcaption> PSNR Comparison against state-of-the-art JDD models on the Urban100/MIT moire datasets</figcaption> </figure> </p> <p>The results of this section are detailed in, <a href="https://arxiv.org/abs/2112.00913"><em>&quot;CDLNet: Noise-Adaptive Convolutional Dictionary Learning Network for Blind Denoising and Demosaicing&quot;</em></a>.</p> <h2 id=progress ><a href="#progress" class=header-anchor >Progress</a></h2> <ul> <li><p><em>In December 2021</em>, we published a journal paper style preprint where we&#39;ve improved our network&#39;s denoising generalization results and extended them to the tasks of joint-denoising-and-demosaicing &#40;JDD&#41; and unsupervised learning. Notably, our network obtains state-of-the-art results in JDD with a single model. The article is available on Arxiv: <a href="https://arxiv.org/abs/2112.00913"><em>&quot;CDLNet: Noise-Adaptive Convolutional Dictionary Learning Network for Blind Denoising and Demosaicing&quot;</em></a>.</p> <li><p><em>In March 2021</em>, we published a conference paper style preprint where we demonstrate that CDLNet&#39;s interpretable construction may be leveraged to yeild denoising robustness in noise-level mismatch between training and inference. The article is available on Arxiv: <a href="https://arxiv.org/abs/2103.04779"><em>&quot;CDLNet: Robust and Interpretable Denoising Through Deep Convolutional Dictionary Learning&quot;</em></a>.</p> <li><p><em>In January 2021</em>, we presented updated results regarding competitive denoising performance against state-of-the-art deep-learning models and leveraging the interpretability of CDLNet to improve generalization in blind denoising. Our poster is available <a href="/assets/dcdl/CDLNetPosterWireless21.pdf">here</a>.</p> <li><p><em>In April 2020</em>, we presented some preliminary results at the NYU Wireless Industrial Affiliates Board Meeting. Check out our poster <a href="/assets/dcdl/CDLNetPosterWireless20.pdf">here</a>.</p> </ul> <div class=page-foot > <div class=copyright > <a href="https://github.com/nikopj">GitHub</a> <br> &copy; Nikola Janjušević. Last modified: December 07, 2021. Website built with <a href="https://github.com/tlienart/Franklin.jl">Franklin.jl</a> and the <a href="https://julialang.org">Julia programming language</a>. </div> </div> </div>