<!doctype html> <html lang=en > <meta charset=UTF-8 > <meta name=viewport  content="width=device-width, initial-scale=1"> <link rel=stylesheet  href="/libs/katex/katex.min.css"> <link rel=stylesheet  href="/css/franklin.css"> <link rel=stylesheet  href="/css/basic.css"> <link rel=icon  href="/assets/favicon.png"> <title>How do black-box denoisers perform at the lower parameter-count regime, anyway?</title> <header> <div class=blog-name ><a href="/">Nikola Janjušević</a></div> <nav> <ul> <li><a href="/teaching">Teaching</a> <li><a href="/publications">Publications</a> <li><a href="/notes">Notes</a> <li><a href="/assets/resume.pdf">Résumé</a> </ul> <img src="/assets/hamburger.svg" id=menu-icon > </nav> </header> <div class=franklin-content > <u>Published ...? This post is in progress.</u> <h1 id=title ><a href="#title" class=header-anchor >How do black-box denoisers perform at the lower parameter-count regime, anyway?</a></h1> So-called interpretably constructed deep neural networks often sell their methods by showing near state-of-the-art performance for only a fraction of the parameter count of black-box networks. However, can we consider these fair comparisons when the number of learned parameter counts are not matched? <h2 id=background ><a href="#background" class=header-anchor >Background</a></h2> <p>In this post, we&#39;re talking about DeNoising neural networks – parameterized functions <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>f</mi><mi mathvariant=normal >Θ</mi></msub></mrow><annotation encoding="application/x-tex">f_\Theta</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class=mord ><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class=msupsub ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.10764em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">Θ</span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> that take an image contaminated with random noise ex. AWGN,</p> <span class=katex-display ><span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML" display=block ><semantics><mrow><mi>y</mi><mo>=</mo><mi>x</mi><mo>+</mo><mi>ν</mi><mo separator=true >,</mo><mtext> </mtext><mi>ν</mi><mo>∼</mo><mi mathvariant=script >N</mi><mo stretchy=false >(</mo><mn>0</mn><mo separator=true >,</mo><msup><mi>σ</mi><mn>2</mn></msup><mo stretchy=false >)</mo><mo separator=true >,</mo></mrow><annotation encoding="application/x-tex">y = x + \nu, ~ \nu \sim \mathcal{N}(0,\sigma^2),</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class=mspace  style="margin-right:0.2777777777777778em;"></span><span class=mrel >=</span><span class=mspace  style="margin-right:0.2777777777777778em;"></span></span><span class=base ><span class=strut  style="height:0.66666em;vertical-align:-0.08333em;"></span><span class="mord mathnormal">x</span><span class=mspace  style="margin-right:0.2222222222222222em;"></span><span class=mbin >+</span><span class=mspace  style="margin-right:0.2222222222222222em;"></span></span><span class=base ><span class=strut  style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord mathnormal" style="margin-right:0.06366em;">ν</span><span class=mpunct >,</span><span class="mspace nobreak"> </span><span class=mspace  style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal" style="margin-right:0.06366em;">ν</span><span class=mspace  style="margin-right:0.2777777777777778em;"></span><span class=mrel >∼</span><span class=mspace  style="margin-right:0.2777777777777778em;"></span></span><span class=base ><span class=strut  style="height:1.1141079999999999em;vertical-align:-0.25em;"></span><span class="mord mathcal" style="margin-right:0.14736em;">N</span><span class=mopen >(</span><span class=mord >0</span><span class=mpunct >,</span><span class=mspace  style="margin-right:0.16666666666666666em;"></span><span class=mord ><span class="mord mathnormal" style="margin-right:0.03588em;">σ</span><span class=msupsub ><span class=vlist-t ><span class=vlist-r ><span class=vlist  style="height:0.8641079999999999em;"><span style="top:-3.113em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class=mclose >)</span><span class=mpunct >,</span></span></span></span></span> <p>and return an estimate of the underlying signal, <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover accent=true ><mi>x</mi><mo>^</mo></mover><mo>=</mo><msub><mi>f</mi><mi mathvariant=normal >Θ</mi></msub><mo stretchy=false >(</mo><mi>y</mi><mo stretchy=false >)</mo></mrow><annotation encoding="application/x-tex">\hat{x} = f_\Theta(y)</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:0.69444em;vertical-align:0em;"></span><span class="mord accent"><span class=vlist-t ><span class=vlist-r ><span class=vlist  style="height:0.69444em;"><span style="top:-3em;"><span class=pstrut  style="height:3em;"></span><span class="mord mathnormal">x</span></span><span style="top:-3em;"><span class=pstrut  style="height:3em;"></span><span class=accent-body  style="left:-0.22222em;"><span class=mord >^</span></span></span></span></span></span></span><span class=mspace  style="margin-right:0.2777777777777778em;"></span><span class=mrel >=</span><span class=mspace  style="margin-right:0.2777777777777778em;"></span></span><span class=base ><span class=strut  style="height:1em;vertical-align:-0.25em;"></span><span class=mord ><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class=msupsub ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.10764em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">Θ</span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.15em;"><span></span></span></span></span></span></span><span class=mopen >(</span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class=mclose >)</span></span></span></span>. These DNNs are often trained by adding noise to a dataset of natural images, for example the Berkely Segmentation Dataset &#40;BSD&#41;, and optimizing their parameters <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant=normal >Θ</mi></mrow><annotation encoding="application/x-tex">\Theta</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:0.68333em;vertical-align:0em;"></span><span class=mord >Θ</span></span></span></span> &#40;&quot;training&quot;&#41; via stochastic gradient descent on the error &#40;loss&#41; between the estimated clean image and the ground truth, <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant=script >L</mi><mo stretchy=false >(</mo><mi>x</mi><mo separator=true >,</mo><mtext> </mtext><msub><mi>f</mi><mi mathvariant=normal >Θ</mi></msub><mo stretchy=false >(</mo><mi>y</mi><mo stretchy=false >)</mo><mo stretchy=false >)</mo></mrow><annotation encoding="application/x-tex">\mathcal{L}(x, \, f_\Theta(y))</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathcal">L</span><span class=mopen >(</span><span class="mord mathnormal">x</span><span class=mpunct >,</span><span class=mspace  style="margin-right:0.16666666666666666em;"></span><span class=mspace  style="margin-right:0.16666666666666666em;"></span><span class=mord ><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class=msupsub ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.10764em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">Θ</span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.15em;"><span></span></span></span></span></span></span><span class=mopen >(</span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class=mclose >))</span></span></span></span>.</p> <h2 id=what_makes_a_fair_comparison ><a href="#what_makes_a_fair_comparison" class=header-anchor >What makes a fair comparison?</a></h2> <p>In a broad sense, the &quot;free variables&quot; of these experiments are </p> <ul> <li><p>training scheme</p> <ul> <li><p>dataset</p> <li><p>learning rate</p> </ul> <li><p>network architecture &#40;i.e. what operations are done inside <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>f</mi><mi mathvariant=normal >Θ</mi></msub></mrow><annotation encoding="application/x-tex">f_\Theta</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class=mord ><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class=msupsub ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.10764em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">Θ</span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>&#41;</p> <ul> <li><p>functional form of layers &#40;ex. <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>x</mi><mrow><mo stretchy=false >(</mo><mi>k</mi><mo>+</mo><mn>1</mn><mo stretchy=false >)</mo></mrow></msup><mo>=</mo><mi>σ</mi><mo stretchy=false >(</mo><mi>W</mi><msup><mi>x</mi><mrow><mo stretchy=false >(</mo><mi>k</mi><mo stretchy=false >)</mo></mrow></msup><mo>+</mo><mi>b</mi><mo stretchy=false >)</mo></mrow><annotation encoding="application/x-tex">x^{(k+1)} = \sigma(Wx^{(k)} + b)</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:0.8879999999999999em;vertical-align:0em;"></span><span class=mord ><span class="mord mathnormal">x</span><span class=msupsub ><span class=vlist-t ><span class=vlist-r ><span class=vlist  style="height:0.8879999999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span><span class="mbin mtight">+</span><span class="mord mtight">1</span><span class="mclose mtight">)</span></span></span></span></span></span></span></span></span><span class=mspace  style="margin-right:0.2777777777777778em;"></span><span class=mrel >=</span><span class=mspace  style="margin-right:0.2777777777777778em;"></span></span><span class=base ><span class=strut  style="height:1.138em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">σ</span><span class=mopen >(</span><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class=mord ><span class="mord mathnormal">x</span><span class=msupsub ><span class=vlist-t ><span class=vlist-r ><span class=vlist  style="height:0.8879999999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span><span class="mclose mtight">)</span></span></span></span></span></span></span></span></span><span class=mspace  style="margin-right:0.2222222222222222em;"></span><span class=mbin >+</span><span class=mspace  style="margin-right:0.2222222222222222em;"></span></span><span class=base ><span class=strut  style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">b</span><span class=mclose >)</span></span></span></span>&#41;</p> <li><p>hyper-parameters &#40;num. layers, num. filters, etc.&#41;</p> </ul> </ul> <p>In the papers cited below, the same dataset &#40;BSD432&#41; is used to train each network. Training schemes may differ, but it is generally assumed that each team has done their best to maximize the performance of their network.</p> <p>Novel functional forms are almost always the meat of an image denoising paper&#39;s proposed methods. The point is to demonstrate a &quot;better&quot; way of understanding denoising &#40;via neural networks&#41;, or showing that a new tool can be used effectively.</p> <p>A simple and convincing experiment to highlight this would be to ,</p> <blockquote> <p>show the same/better performance to previously published methods using a less/equal number of &quot;X&quot;, </p> </blockquote> <p>where &quot;X&quot; is most commonly floating point operations &#40;flops&#41;, unit of time, or learned parameters. </p> <p>Flops and processing time are self explanatory benifits – and may not be equivalent due to parallelizability of the proposed method. Paying attention to learned parameter counts has some tangible benefits in terms of physically storing the neural network in memory, but it can also comment on the effectiveness of the proposed architecture in an <a href="https://en.wikipedia.org/wiki/Occam&#37;27s_razor">Occam&#39;s Razor</a> sense. Here, we equate a low learned parameter count to &quot;simplicity&quot;, and look to methods with greater simplicity for insight. Beware that this connection is vague and should be put in context. </p> <h2 id=the_experiments ><a href="#the_experiments" class=header-anchor >The Experiments</a></h2> <p>The our open access <a href="https://ieeexplore.ieee.org/document/9769957">journal-paper</a> where we propose the <a href="/projects/dcdl">CDLNet</a> architecture, we demonstrate a competitive<sup id="fnref:1"><a href="#fndef:1" class=fnref >[1]</a></sup> denoising performance to interpretably constructed neural networks, distinguished by faster processing time. We consider this a &quot;fair comparison&quot; as CDLNet is trained on the same dataset and has a roughly similar learned parameter count &#40;~64k&#41;. </p> <p>Ok, but state-of-the-art black-box DNNs &#40;ex. DnCNN ~ 550k, FFDNet ~ 480k&#41; have hundreds of thousands of parameters. How do the interpretable networks perform in comparison? <strong>Worse</strong>. But it&#39;s obviously not a fair comparison. And they don&#39;t have to be fair comparisons if they&#39;re exploring the proposed method from a unique perspective<sup id="fnref:2"><a href="#fndef:2" class=fnref >[2]</a></sup> – in this case the black-box methods serve not as competition but as a compass<sup id="fnref:3"><a href="#fndef:3" class=fnref >[3]</a></sup>. Nonetheless, some authors do cite these experiements as justification for the merit of their proposed method. Perhaps they&#39;re implicitly saying, </p> <blockquote> <p>It&#39;s amazing we got this close, with this simple of a model&#33;</p> </blockquote> <p>How close is close? To answer this, I shall fill in a missing piece of the puzzle: <strong>the performance of low learned parameter count black-box models</strong>. Specifally the most popular black-box denosing network, DnCNN.</p> <ul> <li><p>Table of performances</p> <li><p>appendix: table of DnCNN models</p> </ul> <p><table class=fndef  id="fndef:1"> <tr> <td class=fndef-backref ><a href="#fnref:1">[1]</a> <td class=fndef-content >better than some &#40;but not all&#41;, in some &#40;but not all&#41; cases. </table> <table class=fndef  id="fndef:2"> <tr> <td class=fndef-backref ><a href="#fnref:2">[2]</a> <td class=fndef-content >novel methods <strong>are</strong> worth publishing even if they&#39;re not strictly &quot;SOA&quot;. They may even lead to SOA methods if built upon further by other authors. </table> <table class=fndef  id="fndef:3"> <tr> <td class=fndef-backref ><a href="#fnref:3">[3]</a> <td class=fndef-content >not a compass to follow, simply to orient ourselves. A benchmark. </table> </p> <div class=page-foot > <div class=copyright > <!--<a href="https://github.com/nikopj">GitHub</a> <br> --> &copy; <a href="https://github.com/nikopj">Nikola Janjušević</a>. Last modified: 2022-10-26. Built with <a href="https://github.com/tlienart/Franklin.jl">Franklin.jl</a> + <a href="https://julialang.org">Julia</a>. </div> </div> </div>